{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from src.tkns.tokenizer import RNASequenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________\n",
    "## I. RNACentral data inspection and tokenization of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"./data\"\n",
    "seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data = True\n",
    "chunk_size = 10_000\n",
    "\n",
    "\n",
    "if process_data:\n",
    "    dataset = load_dataset(f\"multimolecule/rnacentral.{seq_length}\", cache_dir=local_dir)\n",
    "    # dataset[\"train\"].to_csv(f\"{local_dir}/rnacentral.{seq_length}.csv\", index=False)\n",
    "\n",
    "    unique_nucleotides = set()\n",
    "\n",
    "    # Process the dataset in chunks\n",
    "    train_data = dataset[\"train\"]\n",
    "    for start_idx in range(0, len(train_data), chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, len(train_data))\n",
    "        chunk = train_data[start_idx:end_idx]\n",
    "        sequences = chunk['sequence']\n",
    "\n",
    "        for seq in sequences:\n",
    "            unique_nucleotides.update(seq)\n",
    "        \n",
    "    unique_nucleotides = set([e.upper() for e in unique_nucleotides])\n",
    "    print(\"Unique nucleotides:\", unique_nucleotides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the special tokens to unique nucleotides\n",
    "unique_nucleotides.update([\"[PAD]\", \"[MASK]\"])\n",
    "list_nucleotides = list(unique_nucleotides)\n",
    "list_nucleotides.sort()\n",
    "\n",
    "# Create a token vocabulary from unique nucleotides\n",
    "vocabulary = {token: idx for idx, token in enumerate(list_nucleotides)}\n",
    "\n",
    "# Save the vocabulary to a JSON file\n",
    "with open(f\"{local_dir}/vocabulary.json\", \"w\") as f:\n",
    "    json.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and store sequences in an H5 file for faster training.\n",
    "tokenize = True\n",
    "\n",
    "if tokenize:\n",
    "    dataset = load_dataset(f\"multimolecule/rnacentral.{seq_length}\", cache_dir=local_dir)\n",
    "    tokenizer = RNASequenceTokenizer()\n",
    "\n",
    "    sequences = dataset[\"train\"][\"sequence\"]\n",
    "    rna_types = dataset[\"train\"][\"type\"]    \n",
    "\n",
    "    print(\"Number of sequences: \", len(sequences))\n",
    "    print(\"RNA types: \", len(rna_types))\n",
    "\n",
    "    h5_file_path = f\"{local_dir}/tokenized_sequences.h5\"\n",
    "\n",
    "    with h5py.File(h5_file_path, 'w') as h5f:\n",
    "\n",
    "        tokenized_seqs_ds = h5f.create_dataset('tokenized_sequences', (len(sequences), seq_length), dtype='int8')\n",
    "        seq_types_ds = h5f.create_dataset('sequence_types', (len(sequences),), dtype=h5py.string_dtype())\n",
    "\n",
    "        # Tokenize sequences and store them in the H5 file\n",
    "        for idx, seq in enumerate(tqdm(sequences, desc=\"Tokenizing sequences: \")):\n",
    "            tokenized_seq = tokenizer.encode(seq)\n",
    "            tokenized_seqs_ds[idx, :len(tokenized_seq)] = tokenized_seq\n",
    "            seq_types_ds[idx] = rna_types[idx]\n",
    "\n",
    "    print(f\"Tokenized sequences and sequence types have been stored in {h5_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "split_data = True\n",
    "\n",
    "if split_data:\n",
    "    h5_file_path = f\"{local_dir}/tokenized_sequences.h5\"\n",
    "\n",
    "    # Load the original H5 file\n",
    "    with h5py.File(h5_file_path, 'r') as h5f:\n",
    "        tokenized_sequences = h5f['tokenized_sequences'][:]\n",
    "        sequence_types = h5f['sequence_types'][:]\n",
    "\n",
    "    # Determine the split index\n",
    "    num_sequences = len(tokenized_sequences)\n",
    "    split_index = int(0.9 * num_sequences)\n",
    "\n",
    "    # Shuffle the indices\n",
    "    indices = np.arange(num_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Split the indices into train and test sets\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "\n",
    "    # Create train and test H5 files\n",
    "    train_h5_file_path = f\"{local_dir}/tokenized_sequences_train.h5\"\n",
    "    test_h5_file_path = f\"{local_dir}/tokenized_sequences_test.h5\"\n",
    "\n",
    "    with h5py.File(train_h5_file_path, 'w') as train_h5f, h5py.File(test_h5_file_path, 'w') as test_h5f:\n",
    "        # Create datasets for train and test sets\n",
    "        train_h5f.create_dataset('tokenized_sequences', data=tokenized_sequences[train_indices])\n",
    "        train_h5f.create_dataset('sequence_types', data=sequence_types[train_indices])\n",
    "        \n",
    "        test_h5f.create_dataset('tokenized_sequences', data=tokenized_sequences[test_indices])\n",
    "        test_h5f.create_dataset('sequence_types', data=sequence_types[test_indices])\n",
    "\n",
    "    print(f\"Train and test H5 files have been created: {train_h5_file_path}, {test_h5_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test tokenizer\n",
    "tokenizer = RNASequenceTokenizer()\n",
    "\n",
    "# Encoding and decoding example\n",
    "sequence = \"AAAFCG\" # sequences[0]\n",
    "encoded = tokenizer.encode(sequence)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "print(\"Encoding / decoding: \", sequence == decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________\n",
    "## II. Init dataset, collate function and dataloader. Inspect inputs, masked inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datasets.rna_central import RNACentral\n",
    "from src.datasets.masked_lm import collate_fn_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\"mask_prob\": 0.30, \"no_mask_tokens\": [], \"randomize_prob\": 0.1, \"no_change_prob\": 0.1, \"max_length\": 10, \"batch_size\": 1, \"n_tokens\": 15}\n",
    "\n",
    "sequences = [\"ACGTACGCGTATATTTGGGA\", \"TTAAACCCGGTAACAAAATTTGCGTA\", \"CGTACGTA\", \"ACGTACGT\", \"TTGACGTA\", \"CGTACGTA\"]\n",
    "\n",
    "tokenizer = RNASequenceTokenizer()\n",
    "dataset = RNACentral(lines=sequences, tokenizer=tokenizer, max_length=10)\n",
    "\n",
    "custom_collate_fn = partial(collate_fn_mlm,\n",
    "                            pad_token_id=tokenizer.vocabulary[\"[PAD]\"],\n",
    "                            mask_token_id=tokenizer.vocabulary[\"[MASK]\"],\n",
    "                            mask_prob=config[\"mask_prob\"],\n",
    "                            no_mask_tokens=config[\"no_mask_tokens\"],\n",
    "                            n_tokens=len(tokenizer.vocabulary),\n",
    "                            randomize_prob=config[\"randomize_prob\"],\n",
    "                            no_change_prob=config[\"no_change_prob\"],)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch and demonstrate masking\n",
    "for batch_idx, (masked_input_ids, masked_labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(\"Input sequences:\")\n",
    "    \n",
    "    for seq_idx, (masked_sequence, token_ids) in enumerate(zip(masked_input_ids, dataset)):\n",
    "        \n",
    "        original_sequence = ' '.join(map(str, token_ids.tolist()))\n",
    "        masked_sequence_str = ' '.join(\n",
    "            f\"\\033[31m{token_id}\\033[0m\" if token_id == tokenizer.vocabulary.get(\"[MASK]\", 2) else str(token_id)\n",
    "            for token_id in masked_sequence.tolist()\n",
    "        )\n",
    "        print(f\"\\tOriginal Sequence {seq_idx + 1}: {original_sequence}\")\n",
    "        print(f\"\\tMasked Sequence   {seq_idx + 1}: {masked_sequence_str}\")\n",
    "    \n",
    "    print(\"\\nTarget sequences:\")\n",
    "    for seq_idx, sequence in enumerate(masked_labels):\n",
    "        print(f\"\\tSequence {seq_idx + 1}:      {' '.join(map(str, sequence.tolist()))}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________\n",
    "## Model initialization, training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from src.tkns.tokenizer import RNASequenceTokenizer\n",
    "from src.models.bert.bert import BERT\n",
    "from src.models.bert.cgf import BERTConfig, TrainingConfig\n",
    "from src.datasets.masked_lm import collate_fn_mlm\n",
    "from src.datasets.rna_central import RNACentral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 model: nn.Module, \n",
    "                 train_dataloader: DataLoader, \n",
    "                 val_dataloader: DataLoader,\n",
    "                 tokenizer,\n",
    "                 criterion: nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "                 config: TrainingConfig,\n",
    "                 writer):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.tokenizer = tokenizer\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.config = config\n",
    "        self.writer = writer\n",
    "        self.global_step = 0\n",
    "\n",
    "    def train_epoch(self, epoch: int):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, (input_ids, labels) in enumerate(tqdm(self.train_dataloader, desc=f\"Epoch {epoch + 1} Training\")):\n",
    "            input_ids, labels = input_ids.to(self.config.device), labels.to(self.config.device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(input_ids)[\"logits\"]\n",
    "\n",
    "            # Compute loss (transpose to match CrossEntropyLoss dimensions)\n",
    "            loss = self.criterion(logits.transpose(1, 2), labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
    "\n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Scheduler step\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # Log loss periodically\n",
    "            if self.global_step % self.config.log_steps == 0:\n",
    "                avg_loss = total_loss / (i + 1)\n",
    "                self.writer.add_scalar(\"Loss/Train\", avg_loss, self.global_step)\n",
    "                print(f\"Step {self.global_step} | Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model periodically\n",
    "            if self.global_step % self.config.save_steps == 0:\n",
    "                model_path = f\"model_step_{self.global_step}.pt\"\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "                print(f\"Model saved at step {self.global_step}: {model_path}\")\n",
    "\n",
    "            self.global_step += 1\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def validate_epoch(self, epoch: int):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (input_ids, labels) in tqdm(self.val_dataloader, desc=f\"Epoch {epoch + 1} Validation\"):\n",
    "                input_ids, labels = input_ids.to(self.config.device), labels.to(self.config.device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = self.model(input_ids)[\"logits\"]\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.criterion(logits.transpose(1, 2), labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Log loss periodically during validation\n",
    "                if i % self.config.log_steps == 0:\n",
    "                    avg_loss = total_loss / (i + 1)\n",
    "                    self.writer.add_scalar(\"Loss/Validation\", avg_loss, self.global_step)\n",
    "                    print(f\"Step {self.global_step} | Validation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_loss = self.validate_epoch(epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), \"best_model.pt\")\n",
    "                print(\"Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "tokenizer = RNASequenceTokenizer()\n",
    "\n",
    "\n",
    "\n",
    "# Dataclass for training configurations\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int\n",
    "    lr: float\n",
    "    n_epochs: int\n",
    "    max_seq_length: int\n",
    "    device: str\n",
    "    gradient_clip: float = 1.0\n",
    "    log_steps: int = 500\n",
    "    save_steps: int = 100000\n",
    "    pad_token_id: int = 0\n",
    "    mask_token_id: int = 1\n",
    "    mask_prob: float = 0.15\n",
    "    no_mask_tokens: List[int] = None\n",
    "    n_tokens: int = 0\n",
    "    randomize_prob: float = 0.1\n",
    "    no_change_prob: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BERTConfig(dim=256, n_heads=8, attn_dropout=0.1, mlp_dropout=0.1, depth=6, \n",
    "                         vocab_size=21, max_len=512, pad_token_id=0, mask_token_id=1)\n",
    "\n",
    "train_config = TrainingConfig(\n",
    "    batch_size=32, \n",
    "    lr=6e-5, \n",
    "    n_epochs=2, \n",
    "    max_seq_length=512, \n",
    "    device=\"cuda\", \n",
    "    log_steps=500, \n",
    "    save_steps=100000,\n",
    "    pad_token_id=tokenizer.vocabulary[\"[PAD]\"],\n",
    "    mask_token_id=tokenizer.vocabulary[\"[MASK]\"],\n",
    "    mask_prob=0.15,\n",
    "    no_mask_tokens=[],\n",
    "    n_tokens=len(tokenizer.vocabulary),\n",
    "    randomize_prob=0.1,\n",
    "    no_change_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"./logs/mlm_training\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RNASequenceTokenizer()\n",
    "\n",
    "# Create datasets\n",
    "dataset_train = RNACentral(h5_file_path=\"/home/andrii/Documents/genrna/data/tokenized_sequences_train.h5\", tokenizer=tokenizer, max_length=train_config.max_seq_length)\n",
    "dataset_test = RNACentral(h5_file_path=\"/home/andrii/Documents/genrna/data/tokenized_sequences_test.h5\", tokenizer=tokenizer, max_length=train_config.max_seq_length)\n",
    "\n",
    "# Create custom collate function\n",
    "custom_collate_fn = partial(collate_fn_mlm,\n",
    "                            pad_token_id=tokenizer.vocabulary[\"[PAD]\"],\n",
    "                            mask_token_id=tokenizer.vocabulary[\"[MASK]\"],\n",
    "                            mask_prob=train_config.mask_prob,\n",
    "                            no_mask_tokens=train_config.no_mask_tokens,\n",
    "                            n_tokens=train_config.n_tokens,\n",
    "                            randomize_prob=train_config.randomize_prob,\n",
    "                            no_change_prob=train_config.no_change_prob)\n",
    "\n",
    "# Create data loaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=train_config.batch_size, collate_fn=custom_collate_fn)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=train_config.batch_size, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = BERT(bert_config).to(train_config.device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = Adam(model.parameters(), lr=train_config.lr)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=train_config.lr, steps_per_epoch=len(dataloader_train), epochs=train_config.n_epochs)\n",
    "\n",
    "# Criterion\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocabulary[\"[PAD]\"])\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=dataloader_train,\n",
    "    val_dataloader=dataloader_test,\n",
    "    tokenizer=tokenizer,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=train_config,\n",
    "    writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
